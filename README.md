 
# **Character-Level Text Generation Using LSTM**

## **Overview**
This program demonstrates character-level text generation using a Long Short-Term Memory (LSTM) network implemented with PyTorch. The LSTM model predicts the next character in a sequence and generates new text by iteratively predicting characters based on previously generated ones. 

Key features include:
- **Learning character dependencies** in text sequences.
- Generating new text from a given starting sequence (prime).
- Encoding text as integers and applying one-hot encoding for input representation.

## **Key Features**
1. **LSTM-Based Model:**
   - Designed to capture both short-term and long-term dependencies in text.
   - Includes dropout for regularization.

2. **Character Encoding:**
   - Maps characters to integers and integers to one-hot encoded vectors.

3. **Text Generation:**
   - Generates new text based on an initial sequence (prime).
   - Incorporates randomness using top-k sampling for diversity in output.

4. **Training on Custom Data:**
   - Easily trainable on any text dataset.

---

## **Usage**

### **Requirements**
Install the required libraries before running the code:
```bash
pip install torch numpy
```

### **File Structure**
- `data/anna.txt`: The input text file used for training.
- `text_generator.py`: The main script containing the model and training pipeline.

---

## **Program Workflow**
1. **Text Preprocessing:**
   - Reads text from the file.
   - Creates mappings between characters and integers for encoding/decoding.

2. **Model Definition:**
   - Implements an LSTM-based recurrent neural network.
   - Consists of:
     - LSTM layer for learning sequence dependencies.
     - Dropout layer for regularization.
     - Fully connected layer for predicting character probabilities.

3. **Training the Model:**
   - Divides the dataset into training and validation sets.
   - Encodes input sequences using one-hot encoding.
   - Applies gradient clipping to handle exploding gradients.
   - Tracks training and validation loss.

4. **Text Generation:**
   - Generates text character by character using a prime string.
   - Allows for randomness in predictions using top-k sampling.

---

## **Model Architecture**
| Component         | Description                                      |
|-------------------|--------------------------------------------------|
| **Input Layer**   | One-hot encoded vectors representing characters. |
| **LSTM Layer**    | Learns sequence dependencies with 2 layers and 512 hidden units. |
| **Dropout Layer** | Regularization to prevent overfitting (50%).     |
| **Fully Connected Layer** | Maps LSTM output to character probabilities. |

---

## **How to Run**

### **1. Prepare the Text Data**
Place the text data you want to train on in the `data/anna.txt` file. 

### **2. Train the Model**
Run the script to train the model:
```bash
python text_generator.py
```
You can adjust hyperparameters (e.g., number of epochs, batch size, learning rate) as needed:
```python
n_hidden = 512
n_layers = 2
batch_size = 64
seq_length = 100
n_epochs = 3
```

### **3. Generate Text**
After training, the script generates new text based on a prime sequence:
```python
print(sample(net, 1000, prime='Anna', top_k=5))
```

---

## **Customization**
1. **Hyperparameters:**
   Adjust model hyperparameters such as:
   - Number of hidden units in the LSTM (`n_hidden`)
   - Number of layers in the LSTM (`n_layers`)
   - Sequence length (`seq_length`)
   - Learning rate (`lr`)

2. **Dataset:**
   Replace `data/anna.txt` with any text file to train the model on custom data.

3. **Sampling Diversity:**
   Use the `top_k` parameter in the `sample` function to control randomness in text generation.

---

## **Results**
1. **Training Loss:**
   - The program prints training and validation loss at regular intervals during training.

2. **Generated Text:**
   - Example output for a prime sequence (`Anna`):
     ```
     Anna is a great name. This is a sample text generated by the model...
     ```

---

## **Contact**
**Author**: Elian Iluk  
**Email**: elian10119@gmail.com  

Feel free to reach out for any questions or feedback regarding the program.

